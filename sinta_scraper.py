# -*- coding: utf-8 -*-
"""sinta_scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K6InMIupm9_6RMTHXRASkxQ1OuIOqV8P
"""

import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import time
import io
import requests
import urllib.request
import json
from urllib.parse import quote
import PyPDF2  #pip install PyPDF2
from unicodedata import normalize
from langdetect import detect  #pip install langdetect

class Sinta(object):

    # Constructor
    def __init__(self, username='username', password='password'):
        self.username = username
        self.password = password

        BASE_URL = 'https://sinta.kemdikbud.go.id/authorverification/login/do_login'
        session = requests.session()
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
        session.post(BASE_URL, {"username": username, "password": password}, headers=headers)

        self.session = session
    
    def login_info(self,session_sinta):
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
        DASHBOARD_URL = 'https://sinta.kemdikbud.go.id/authorverification/dashboard'
        res = ''
        try:
          response = session_sinta.get(DASHBOARD_URL, headers=headers)
          soup = BeautifulSoup(response.text, 'html.parser')
          try:
            account = soup.find('main').find('a').find('span').text
            status = 'Login success. Session start !'
            login = True
          except Exception:
            account = 'no account because login failed'
            status = 'login failed! session expired ! Please login again with correct username and password !'
            login = False
          res = {'account': account,
                  'status' : status,
                  'login' : login}
        except Exception:
          res = 'failed to request web page'
        return res

    # Method Scrap Link Author Sinta
    def scrap_link_author_sinta(self,session_sinta):
        AUTHOR_URL = 'https://sinta.kemdikbud.go.id/authorverification/author'
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}

        try:
          response = session_sinta.get(AUTHOR_URL, headers=headers)
          soup = BeautifulSoup(response.text, 'html.parser')
          page = soup.find("div",{"class": "col-md-6 mb-2 text-lg-left text-center light-font"}).find("small").text
          total_page = int(re.findall(r'\d+', page)[1])
          # total_page

          link_list = []

          for page in range(0,total_page):
            page = page+1
            AUTHOR_PAGE_URL = "https://sinta.kemdikbud.go.id/authorverification/author?page="+str(page)

            try:
              response = session_sinta.get(AUTHOR_PAGE_URL, headers=headers)
              soup = BeautifulSoup(response.text, 'html.parser')
              tr_author = soup.find('tbody').find_all('tr')

              for i in tr_author:
                class_author = i.find('td',{'class': 'align-middle no-wrap'})

                # get link
                try:
                  link = class_author.find("a")['href']
                except Exception:
                  link = ''
                link_list.append(link)

            except Exception:
              link_list.append('')
          return(link_list)
        except Exception:
          res = 'session is expired! login required!'
          return res

    # Method Scrap Author Sinta
    def scrap_author_sinta(self,session_sinta):
        AUTHOR_URL = 'https://sinta.kemdikbud.go.id/authorverification/author'
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}

        try:
          response = session_sinta.get(AUTHOR_URL, headers=headers)
          soup = BeautifulSoup(response.text, 'html.parser')
          page = soup.find("div",{"class": "col-md-6 mb-2 text-lg-left text-center light-font"}).find("small").text
          total_page = int(re.findall(r'\d+', page)[1])
          # total_page

          link_list = []
          fname_list = []
          email_list = []
          author_id_list = []
          NIDN_list = []

          for page in range(0,total_page):
            page = page+1
            AUTHOR_PAGE_URL = "https://sinta.kemdikbud.go.id/authorverification/author?page="+str(page)

            try:
              response = session_sinta.get(AUTHOR_PAGE_URL, headers=headers)
              soup = BeautifulSoup(response.text, 'html.parser')
              tr_author = soup.find('tbody').find_all('tr')

              for i in tr_author:
                class_author = i.find('td',{'class': 'align-middle no-wrap'})

                # get link
                try:
                  link = class_author.find("a")['href']
                except Exception:
                  link = ''
                link_list.append(link)

                # get fullname author
                try:
                  fullname = class_author.find("a").text
                except Exception:
                  fullname=''
                fname_list.append(fullname)

                # get email author
                try:
                  email = class_author.find('span').get_text(strip=True, separator='\n').split('\n')[1]
                except Exception:
                  email=''
                email_list.append(email)

                string_id_nidn = class_author.find('span').get_text()

                # get author ID
                try:
                  author_id = re.findall(r'\d+', string_id_nidn)[0]
                except Exception:
                  author_id=''
                author_id_list.append(author_id)

                # get author NIDN
                try:
                  NIDN = re.findall(r'\d+', string_id_nidn)[1]
                except Exception:
                  NIDN = ''
                NIDN_list.append(NIDN)


            except Exception:
              res = 'session is expired! login required!'
          df = pd.DataFrame({'link': link_list,
                            'fullname': fname_list,
                            'email': email_list,
                            'author_id': author_id_list,
                            'NIDN': NIDN_list})
          return(df)

        except Exception:
          res = 'session is expired! login required!'
          return res

    # Method Scrap Scopus Sinta
    def scrap_scopus(self, session_sinta, yf=0, yl=0):

        url_author=self.scrap_link_author_sinta(session_sinta)

        author_list = []
        q_list = []
        link_list = []
        scopus_id_list = []
        title_list = []
        creator_list = []
        journal_list = []
        type_list = []
        year_list = []
        cited_list = []

        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
        try:
          for i in range(0,len(url_author)):
            url = url_author[i]+'?view=scopus'
            try:
              response = session_sinta.get(url, headers=headers)
              soup = BeautifulSoup(response.text, 'html.parser')
              author = soup.find('div',{'class':'media-body'}).find('h5').text
              page = soup.find("div",{"class": "col-md-6 text-center text-lg-left light-font mb-3"}).find('small').text
              total_page = int(re.findall(r'\d+', page)[1])
              # total_page

              for page in range(0,total_page):
                page = page+1
                url = url_author[i]+'?page='+str(page)+'&view=scopus'
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
                response = session_sinta.get(url, headers=headers)
                soup = BeautifulSoup(response.text, 'html.parser')
                row_content = soup.find('table').find_all('tr')

                for row in row_content:
                  try:
                    year = row.find_all('td')[2].find('small').find_all('strong')[1].text
                  except Exception:
                    year=''
                  # range yf-yl
                  if (yf!=0 and yl!=0) and (year!='' and (int(year)<yf or int(year)>yl)):
                    if int(year)<yf:
                      break
                    continue
                  # yf-last year
                  elif (yf!=0 and yl==0) and (year!='' and int(year)<yf):
                    break
                  # 0-yl
                  elif (yf==0 and yl!=0) and (year!='' and int(year)>yl):
                    continue

                  try:
                    quartile = row.find_all('td')[0].find('div').get_text(strip=True)
                  except Exception:
                    quartile=''
                  try:
                    link = row.find_all('td')[1].find('a')['href']
                    scopus_id = link.split('0-')[1].split('&')[0]
                  except Exception:
                    link=''
                    scopus_id=''
                  try:
                    title = row.find_all('td')[1].find('a').text
                  except Exception:
                    title=None
                  try:
                    creator = row.find_all('td')[1].find_all('small')[0].text
                  except Exception:
                    creator=''
                  try:
                    journal = row.find_all('td')[1].find_all('small')[1].text
                  except Exception:
                    journal=''
                  try:
                    type_journal = row.find_all('td')[2].find('small').find_all('strong')[0].text
                    if type_journal.lower() == 'conference proceedin':
                      type_journal = 'conference proceeding article'
                    elif type_journal.lower() == 'journal':
                      type_journal = 'journal article'
                    type_journal = type_journal.lower()
                  except Exception:
                    type_journal=''
                  try:
                    cited = row.find_all('td')[3].find('small').find_all('strong')[0].text
                  except Exception:
                    cited=''

                  author_list.append(normalize('NFKD', author))
                  q_list.append(quartile)
                  link_list.append(link)
                  scopus_id_list.append(scopus_id)
                  title_list.append(normalize('NFKD', title))
                  creator_list.append(creator)
                  journal_list.append(normalize('NFKD', journal))
                  type_list.append(type_journal)
                  year_list.append(year)
                  cited_list.append(cited)

                else:
                  continue
                break
              successful = True
            except Exception:
              successful = False
            if successful == False:
              continue
          df = pd.DataFrame({'author_scopus':author_list,
                            'title': title_list,
                            'creator_scopus': creator_list,
                            'journal_scopus': journal_list,
                            'type_paper': type_list,
                            'rank_scopus': q_list,
                            'link_scopus': link_list,
                            'scopus_id': scopus_id_list,
                            'year_scopus':year_list,
                            'cited_scopus':cited_list})
          return(df)
        except Exception:
          res = 'session is expired! login required!'
          return res

    # Method Scrap WoS Sinta
    def scrap_wos(self, session_sinta, yf=0, yl=0):
        url_author=self.scrap_link_author_sinta(session_sinta)

        author_list = []
        rank_list = []
        link_list = []
        title_list = []
        authors_list = []
        journal_list = []
        year_list = []
        cited_list = []

        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
        try:
          for i in range(0,len(url_author)):
            url = url_author[i]+'?view=wos'
            try:
              response = session_sinta.get(url, headers=headers)
              soup = BeautifulSoup(response.text, 'html.parser')
              author = soup.find('div',{'class':'media-body'}).find('h5').text
              page = soup.find("div",{"class": "col-md-6 text-center text-lg-left light-font mb-3"}).find('small').text
              total_page = int(re.findall(r'\d+', page)[1])
              # total_page

              for page in range(0,total_page):
                page = page+1
                url = url_author[i]+'?page='+str(page)+'&view=wos'
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
                response = session_sinta.get(url, headers=headers)
                soup = BeautifulSoup(response.text, 'html.parser')
                row_content = soup.find('table').find_all('tr')

                for row in row_content:
                  try:
                    year = row.find_all('td')[2].find('small').find('strong').text
                    year = year.split(' ')[-1]
                  except Exception:
                    year=''

                  # range yf-yl
                  if (yf!=0 and yl!=0) and (year!='' and (int(year)<yf or int(year)>yl)):
                    continue
                  # yf-last year
                  elif (yf!=0 and yl==0) and (year!='' and int(year)<yf):
                    continue
                  # 0-yl
                  elif (yf==0 and yl!=0) and (year!='' and int(year)>yl):
                    continue

                  try:
                    rank = row.find_all('td')[0].find('div').get_text(strip=True)
                  except Exception:
                    rank=''
                  try:
                    link = row.find_all('td')[1].find('a')['href']
                  except Exception:
                    link=''
                  try:
                    title = row.find_all('td')[1].find('a').text
                  except Exception:
                    title=None
                  try:
                    authors = row.find_all('td')[1].find_all('small')[0].text.split('Authors : ')[1]
                  except Exception:
                    authors=''
                  try:
                    journal = row.find_all('td')[1].find_all('small')[1].text
                  except Exception:
                    journal=''
                  try:
                    cited = row.find_all('td')[3].find('small').find_all('strong')[0].text
                  except Exception:
                    cited=''

                  author_list.append(normalize('NFKD', author))
                  rank_list.append(rank)
                  link_list.append(link)
                  title_list.append(normalize('NFKD', title))
                  authors_list.append(normalize('NFKD', authors))
                  journal_list.append(normalize('NFKD', journal))
                  year_list.append(year)
                  cited_list.append(cited)

              successful = True
            except Exception:
              successful = False
            if successful == False:
              continue
          df = pd.DataFrame({'author_wos':author_list,
                            'title': title_list,
                            'authors_wos': authors_list,
                            'journal_wos': journal_list,
                            'rank_wos': rank_list,
                            'link_wos': link_list,
                            'year_wos':year_list,
                            'cited_wos':cited_list})
          return(df)
        except Exception:
          res = 'session is expired! login required!'
          return res

    # Method Scrap Garuda Sinta
    def scrap_garuda(self, session_sinta, yf=0, yl=0):
        url_author=self.scrap_link_author_sinta(session_sinta)

        author_list = []
        rank_list = []
        link_list = []
        title_list = []
        authors_list = []
        pub_list = []
        journal_list = []
        doi_list = []
        year_list = []

        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
        try:
          for i in range(0,len(url_author)):
            url = url_author[i]+'?view=garuda'
            try:
              response = session_sinta.get(url, headers=headers)
              soup = BeautifulSoup(response.text, 'html.parser')
              author = soup.find('div',{'class':'media-body'}).find('h5').text
              page = soup.find("div",{"class": "col-md-6 text-center text-lg-left light-font mb-3"}).find('small').text
              total_page = int(re.findall(r'\d+', page)[1])
              # total_page

              for page in range(0,total_page):
                page = page+1
                url = url_author[i]+'?page='+str(page)+'&view=garuda'
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
                response = session_sinta.get(url, headers=headers)
                soup = BeautifulSoup(response.content.decode('utf-8'), 'html.parser')
                row_content = soup.find('table').find_all('tr')

                for row in row_content:
                  try:
                    year = row.find_all('td')[2].find('small').find('strong').text
                  except Exception:
                    year=''

                  # range yf-yl
                  if (yf!=0 and yl!=0) and (year!='' and (int(year)<yf or int(year)>yl)):
                    if int(year)<yf:
                      break
                    continue
                  # yf-last year
                  elif (yf!=0 and yl==0) and (year!='' and int(year)<yf):
                    break
                  # 0-yl
                  elif (yf==0 and yl!=0) and (year!='' and int(year)>yl):
                    continue

                  try:
                    rank = row.find_all('td')[0].find('div').get_text(strip=True)
                  except Exception:
                    rank=''
                  try:
                    link = row.find_all('td')[1].find('a')['href']
                  except Exception:
                    link=''
                  try:
                    title = row.find_all('td')[1].find('a').text
                    title = normalize('NFKD', title)
                    title = re.sub('â\x80\x93','–',title)
                    title = re.sub('Ã¢â\x82¬â\x80\x9c','–',title)
                  except Exception:
                    title=None
                  try:
                    authors = row.find_all('td')[1].find_all('small')[1].text
                  except Exception:
                    authors=''
                  try:
                    pub = row.find_all('td')[1].find_all('small')[0].text
                  except Exception:
                    pub=''
                  try:
                    journal = row.find_all('td')[1].find_all('small')[2].text
                    journal = re.split(' Vol', journal)[0]
                  except Exception:
                    journal=''
                  try:
                    doi = row.find_all('td')[1].find_all('small')[3].text.split('DOI: ')[1]
                  except Exception:
                    doi=None

                  author_list.append(normalize('NFKD', author))
                  rank_list.append(rank)
                  link_list.append(link)
                  title_list.append(normalize('NFKD', title))
                  authors_list.append(normalize('NFKD', authors))
                  pub_list.append(normalize('NFKD', pub))
                  journal_list.append(normalize('NFKD', journal))
                  doi_list.append(doi)
                  year_list.append(year)
                
                else:
                  continue
                break
              successful = True
            except Exception:
              successful = False
            if successful == False:
              continue
          df = pd.DataFrame({'author_garuda':author_list,
                            'title': title_list,
                            'authors_garuda': authors_list,
                            'publisher': pub_list,
                            'journal_garuda': journal_list,
                            'rank_garuda': rank_list,
                            'link_garuda': link_list,
                            'doi_garuda':doi_list,
                            'year_garuda':year_list})
          return(df)
        except Exception:
          res = 'session is expired! login required!'
          return res

    # Method Scrap Google Sinta
    def scrap_google(self, session_sinta, yf=0, yl=0):
        url_author=self.scrap_link_author_sinta(session_sinta)

        author_list = []
        link_list = []
        title_list = []
        authors_list = []
        journal_list = []
        year_list = []
        cited_list = []

        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
        try:
          for i in range(0,len(url_author)):
            url = url_author[i]+'?view=google'
            try:
              response = session_sinta.get(url, headers=headers)
              soup = BeautifulSoup(response.text, 'html.parser')
              author = soup.find('div',{'class':'media-body'}).find('h5').text
              page = soup.find("div",{"class": "col-md-6 text-center text-lg-left light-font mb-3"}).find('small').text
              total_page = int(re.findall(r'\d+', page)[1])
              # total_page

              for page in range(0,total_page):
                page = page+1
                url = url_author[i]+'?page='+str(page)+'&view=google'
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
                response = session_sinta.get(url, headers=headers)
                soup = BeautifulSoup(response.text, 'html.parser')
                row_content = soup.find('table').find_all('tr')

                for row in row_content:
                  try:
                    year = row.find_all('td')[1].find('small').find('strong').text
                  except Exception:
                    year=''
                    
                  # range yf-yl
                  if (yf!=0 and yl!=0) and (year!='' and (int(year)<yf or int(year)>yl)):
                    if int(year)<yf:
                      break
                    continue
                  # yf-last year
                  elif (yf!=0 and yl==0) and (year!='' and int(year)<yf):
                    break
                  # 0-yl
                  elif (yf==0 and yl!=0) and (year!='' and int(year)>yl):
                    continue

                  try:
                    link = row.find_all('td')[0].find('a')['href']
                  except Exception:
                    link=''
                  try:
                    title = row.find_all('td')[0].find('a').text
                  except Exception:
                    title=None
                  try:
                    authors = row.find_all('td')[0].find_all('small')[0].text.split('Author : ')[1]
                  except Exception:
                    authors=''
                  try:
                    journal = row.find_all('td')[0].find_all('small')[1].text
                    journal = re.split(', '+r'(\d+)', journal)[0]
                    if bool(re.search('the '+r'(\d+)', journal.lower()))==False:
                      journal = re.split(' '+r'(\d+)', journal)[0]
                  except Exception:
                    journal=''
                  try:
                    cited = row.find_all('td')[2].find('small').find('strong').text
                  except Exception:
                    cited=''

                  author_list.append(normalize('NFKD', author))
                  link_list.append(link)
                  title_list.append(normalize('NFKD', title))
                  authors_list.append(normalize('NFKD', authors))
                  journal_list.append(normalize('NFKD', journal))
                  year_list.append(year)
                  cited_list.append(cited)

                else:
                  continue
                break
              
            #   if yf!=0:
            #     for page in range(total_page,-1,-1):
            #       stop=False
            #       url = url_author[i]+'?page='+str(page)+'&view=google'
            #       headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
            #       response = session_sinta.get(url, headers=headers)
            #       soup = BeautifulSoup(response.text, 'html.parser')
            #       row_content = soup.find('table').find_all('tr')

            #       for row in row_content:
            #         try:
            #           year = row.find_all('td')[1].find('small').find('strong').text
            #         except Exception:
            #           year=''

            #         if str(year)!='0000':
            #           stop=True
            #           continue

            #         try:
            #           link = row.find_all('td')[0].find('a')['href']
            #         except Exception:
            #           link=''
            #         try:
            #           title = row.find_all('td')[0].find('a').text
            #         except Exception:
            #           title=None
            #         try:
            #           authors = row.find_all('td')[0].find_all('small')[0].text.split('Author : ')[1]
            #         except Exception:
            #           authors=''
            #         try:
            #           journal = row.find_all('td')[0].find_all('small')[1].text
            #         except Exception:
            #           journal=''
            #         try:
            #           cited = row.find_all('td')[2].find('small').find('strong').text
            #         except Exception:
            #           cited=''

            #         author_list.append(author)
            #         link_list.append(link)
            #         title_list.append(title)
            #         authors_list.append(authors)
            #         journal_list.append(journal)
            #         year_list.append(year)
            #         cited_list.append(cited)

            #       if stop==True:
            #         break
   
              successful = True
            except Exception:
              successful = False
            if successful == False:
              continue
          df = pd.DataFrame({'author_google':author_list,
                            'title': title_list,
                            'authors_google': authors_list,
                            'journal_google': journal_list,
                            'link_google': link_list,
                            'year_google':year_list,
                            'cited_google': cited_list})
          return(df)
        except Exception:
          res = 'session is expired! login required!'
          return res
    

class ResearchScraper(object):

    # Constructor
    def __init__(self,
                 API_KEY_ELSEVIER='',
                 API_KEY_SEMANTIC='',
                 API_KEY_SPRINGER='',
                 API_KEY_GOOGLE=''):

        self.API_KEY_ELSEVIER = API_KEY_ELSEVIER
        self.API_KEY_SEMANTIC = API_KEY_SEMANTIC
        self.API_KEY_SPRINGER = API_KEY_SPRINGER
        self.API_KEY_GOOGLE = API_KEY_GOOGLE

    # Method Scraping Abstract From PDF
    def between(self,value, a, b):
        # Find and validate before-part.
        pos_a = value.find(a)
        if pos_a == -1: return None
        # Find and validate after part.
        pos_b = value.rfind(b)
        if pos_b == -1: return None
        # Return middle part.
        adjusted_pos_a = pos_a + len(a)
        if adjusted_pos_a >= pos_b: return None
        return value[adjusted_pos_a:pos_b]

    def abstract_pdf(self,pdf):
      try:
        r = requests.get(pdf)
        f = io.BytesIO(r.content)

        reader = PyPDF2.PdfReader(f)
        page = reader.pages[0].extract_text()
        abstract = self.between(page,"Abstract","Keywords")
        if abstract==None:
          abstract = self.between(page,"Abstrak","Kata Kunci")
        if abstract==None:
          abstract = self.between(page,"Abstract","Introduction")
        if abstract==None:
          abstract = self.between(page,"Abstrak","Pendahuluan")

        try:
          abstract = abstract.replace("\n", "")
        except Exception:
          abstract=None

      except Exception:
        abstract=None

      return abstract

    # Method Scraping Abstract From URL
    def abstract_url(self,url):
      try:
        abstract=None
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2810.1 Safari/537.36'}
        response = requests.get(url,headers=headers)
        myPage = response.content
        # myPage
        list_index=[]
        for line in myPage.splitlines():
            if b'Abstract' in line:
                index_abstract = myPage.splitlines().index(line)
                list_index.append(index_abstract)
                index_abstract = myPage.splitlines().index(line)+1
                list_index.append(index_abstract)
                # print(myPage.splitlines()[index_abstract])
                # print(index_abstract)
        for index in list_index:
          content = myPage.splitlines()[index].decode("utf-8")
          abstract = BeautifulSoup(content).text
          while abstract=='':
            index +=1
            content = myPage.splitlines()[index].decode("utf-8")
            abstract = BeautifulSoup(content).text
          if len(abstract)<100:
            abstract=None
            continue
          else:
            break
      except Exception:
        abstract=None

      return abstract

    # API ELSEVIER
    def api_elsevier(self,scopus_id):
      # Scraping from API Elsivier just for scopus data (with scopus id)
      params = {'apikey': self.API_KEY_ELSEVIER, 'httpAccept': 'application/json'}
      try:
        # full article
        r = requests.get("https://api.elsevier.com/content/article/scopus_id/"+scopus_id+"", params=params)
        js = r.json()
        # publisher and alternative doi
        r2 = requests.get("https://api.elsevier.com/content/abstract/scopus_id/"+scopus_id+"", params=params)
        js2 = r2.json()

        # DOI
        try:
          doi = js['full-text-retrieval-response']['coredata']['dc:identifier'].split('doi:')[1]
        except Exception:
            try:
              doi = js2['abstracts-retrieval-response']['coredata']['prism:doi']
            except Exception:
              doi=None

        # PUBLISHER
        try:
          publisher = js2['abstracts-retrieval-response']['coredata']['dc:publisher']
        except Exception:
          publisher=None

        # ABSTRACT
        try:
          abstract = js['full-text-retrieval-response']['coredata']['dc:description']
          abstract = abstract.strip()
          abstract = normalize('NFKD', abstract)
        except Exception:
          abstract=None

        # AUTHORS
        try:
          creators = js['full-text-retrieval-response']['coredata']['dc:creator']
          authors=[]
          for i in range(0, len(creators)):
            author = creators[i]['$']
            authors.append(author)
        except Exception:
          authors=None

      except Exception:
        abstract=None
        authors=None
        doi=None
        publisher=None

      data = {
          "abstract": abstract,
          "authors": authors,
          "doi": doi,
          "publisher": publisher
      }
      return(data)

    # API UNPAYWALL
    def api_unpaywall(self,type_search,keyword):
      doi=None
      authors=None
      publication_name=None
      publisher=None
      type_paper=None
      url_for_pdf=None
      url_for_landing_page=None

      if type_search=='doi':
        url = "https://api.unpaywall.org/v2/"+keyword+"?email=unpaywall_01@example.com"
      elif type_search=='title':
        title_quote = quote(keyword, safe='/', encoding=None, errors=None)
        url = "https://api.unpaywall.org/v2/search?query="+title_quote+"&email=unpaywall_01@example.com"
      try:
        f = urllib.request.urlopen(url)
        htmlSource = f.read().decode('utf-8')
        f.close()
        json_file = json.loads(htmlSource)
        if type_search=='title':
          json_file = json_file['results'][0]['response']
        if type_search=='doi' or json_file['title'].lower()==keyword.lower():
          # DOI
          try:
            doi = json_file['doi']
          except Exception:
            doi=None
          # AUTHORS
          try:
            z_authors = json_file['z_authors']
            authors = []
            for j in range(0,len(z_authors)):
              author = z_authors[j]['given']+' '+z_authors[j]['family']
              authors.append(author)
          except Exception:
            authors=None
          # PUBLICATION NAME
          try:
            publication_name = BeautifulSoup(json_file['journal_name']).text 
          except Exception:
            publication_name=None
          # PUBLISHER
          try:
            publisher = json_file['publisher']
          except Exception:
            publisher=None
          # TYPE PAPER
          try:
            type_paper = json_file['genre']
          except Exception:
            type_paper = None
          # URL PDF
          try:
            url_for_pdf = json_file['best_oa_location']['url_for_pdf']
          except Exception:
            url_for_pdf=None
          # URL LANDING PAGE
          try:
            url_for_landing_page = json_file['best_oa_location']['url_for_landing_page']
          except Exception:
            url_for_landing_page=None

      except Exception:
        doi=None
        authors=None
        publication_name=None
        publisher=None
        type_paper=None
        url_for_pdf=None
        url_for_landing_page=None

      data = {
          "authors": authors,
          "doi": doi,
          "publication_name": publication_name,
          "publisher": publisher,
          "type_paper": type_paper,
          "url_for_pdf": url_for_pdf,
          "url_for_landing_page": url_for_landing_page
      }
      return(data)

    #API SPRINGER
    def api_springer(self,type_search,keyword):
      abstract=None
      authors=None
      doi=None
      publication_name=None
      publisher=None
      type_paper=None
      headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}
      if type_search=='doi':
        url = 'https://api.springernature.com/meta/v2/json?q=doi:'+str(keyword)+'&api_key='+self.API_KEY_SPRINGER #add self
      elif type_search=='title':
        url = 'http://api.springernature.com/meta/v2/json?q=(title:"'+str(keyword)+'")&api_key='+self.API_KEY_SPRINGER #add self
      try:
        response = requests.get(url,headers=headers)
        result = response.json()
        if type_search=='doi' or result['records'][0]['title'].lower()==keyword.lower():
          # ABSTRACT
          try:
            abstract = result['records'][0]['abstract']
            abstract = normalize('NFKD', abstract)
          except Exception:
            abstract=None
          # DOI
          try:
            doi = result['records'][0]['doi']
          except Exception:
            doi=None
          # AUTHORS
          try:
            creators = result['records'][0]['creators']
            authors = []
            for i in range(0,len(creators)):
              authors.append(creators[i]['creator'])
          except Exception:
            authors=None
          # PUBLICATION NAME
          try:
            publication_name = result['records'][0]['publicationName']
          except Exception:
            publication_name=None
          # PUBLISHER
          try:
            publisher = result['records'][0]['publisherName']
          except Exception:
            publisher=None
          # TYPE PAPER
          try:
            type_paper = result['records'][0]['publicationType']
          except Exception:
            type_paper=None

      except Exception:
        abstract=None
        authors=None
        doi=None
        publication_name=None
        publisher=None
        type_paper=None

      data = {
          "abstract": abstract,
          "doi": doi,
          "authors": authors,
          "publication_name": publication_name,
          "publisher": publisher,
          "type_paper": type_paper
      }
      return(data)

    # API CROSSREF
    def api_crossref(self,type_search,keyword):
      abstract=None
      authors=None
      doi=None
      publication_name=None
      publisher=None
      type_paper=None

      if type_search=='doi':
        url = 'https://api.crossref.org/works/'+keyword
      elif type_search=='title':
        title_quote = quote(keyword, safe='/', encoding=None, errors=None)
        url = 'https://api.crossref.org/works?query.title='+title_quote+'&rows=1'
      try:
        f = urllib.request.urlopen(url)
        htmlSource = f.read().decode('utf-8')
        f.close()
        result = json.loads(htmlSource)
        if type_search=='doi':
          result = result['message']
        elif type_search=='title':
          result = result['message']['items'][0]
        if type_search=='doi' or result['title'][0].lower()==keyword.lower():
          # ABSTRACT
          try:
            abstract = result['abstract']
            soup = BeautifulSoup(abstract)
            abstract = soup.get_text()
            abstract = re.sub(r'<.*?>', '', abstract)
            abstract = re.sub('\n               ', '', abstract)
            abstract = re.sub('\n', ' ', abstract)
            abstract = normalize('NFKD', abstract)
          except Exception:
            abstract=None
          # DOI
          try:
            doi = result['DOI']
          except Exception:
            doi=None
          # AUTHORS
          try:
            author_json = result['author']
            authors = []
            for j in range(0,len(author_json)):
              try:
                author = author_json[j]['given']+' '+author_json[j]['family']
              except Exception:
                author = author_json[j]['family']
              authors.append(author)
          except Exception:
            authors=None
          # PUBLICATION_NAME
          try:
            publication_name = result['container-title'][0]
          except Exception:
            publication_name=None
          # PUBLISHER
          try:
            publisher = result['publisher']
          except Exception:
            publisher=None
          # TYPE PAPER
          try:
            type_paper = result['type']
          except Exception:
            type_paper=None
      except Exception:
        abstract=None
        authors=None
        doi=None
        publication_name=None
        publisher=None
        type_paper=None

      data = {
          "abstract": abstract,
          "doi": doi,
          "authors": authors,
          "publication_name": publication_name,
          "publisher": publisher,
          "type_paper": type_paper
      }
      return(data)
    
    # API SEMANTIC
    def api_semantic(self,type_search,keyword,year=None):
      abstract=None
      doi=None
      authors=None
      publication_name=None
      cited=None
      type_paper=None
      # use doi (limit 1: just get 1'st paper)
      headers = {'X-API-KEY': self.API_KEY_SEMANTIC, 'httpAccept': 'application/json'} # add self
      if type_search=='doi':
        url="https://api.semanticscholar.org/graph/v1/paper/"+str(keyword)+"?fields=title,url,abstract,authors,externalIds,journal,citationCount,publicationTypes"
      elif type_search=='title':
        if year=='0000' or year=='' or year==None:
          url = "https://api.semanticscholar.org/graph/v1/paper/search?query=title:("+str(keyword)+")&limit=1&fields=title,url,abstract,authors,externalIds,journal,citationCount,publicationTypes"
        else:
          year = str(year)+'-'+str(year)
          url = "https://api.semanticscholar.org/graph/v1/paper/search?query=title:("+str(keyword)+")&year="+str(year)+"&limit=1&fields=title,url,abstract,authors,externalIds,journal,citationCount,publicationTypes"
      try:
        response=requests.get(url,headers=headers)
        response.raise_for_status()
        result = response.json()
        if type_search=='title':
          result = result['data'][0]
        if type_search=='doi' or re.sub("’","'",result['title']).lower()==re.sub("’","'",keyword).lower():
          # ABSTRACT
          try:
            abstract = result['abstract'].strip()
            abstract = normalize('NFKD', abstract)
          except Exception:
            abstract=None
          # DOI
          try:
            doi = result['externalIds']['DOI']
            if doi=='':
              doi=None
          except Exception:
            doi=None
          # AUTHORS
          try:
            creators = result['authors']
            authors=[]
            for i in range(0, len(creators)):
              author = creators[i]['name']
              authors.append(author)
          except Exception:
            authors=None
          # PUBLICATION_NAME
          try:
            publication_name = result['journal']['name']
            if publication_name=='':
              publication_name=None
          except Exception:
            publication_name=None
          # CITED
          try:
            cited = result['citationCount']
          except Exception:
            cited=None
          # TYPE PAPER
          try:
            type_paper = result['publicationTypes'][0]
          except Exception:
            type_paper=None
      except Exception:
        abstract=None
        doi=None
        authors=None
        publication_name=None
        cited=None
        type_paper=None

      data = {
          "abstract": abstract,
          "doi": doi,
          "authors": authors,
          "publication_name": publication_name,
          "cited": cited,
          "type_paper": type_paper
      }
      return(data)

    # COMPLETE SCOPUS
    def complete_scopus(self,df):
      df = df.loc[df['title'].notnull()].reset_index(drop=True)
      df['abstract']=None
      df['authors']=None
      df['doi']=None
      df['publisher']=None
      df['abstract_info']=None
      df['doi_info']=None
    #   df['url_for_pdf']=None
    #   df['url_for_landing_page']=None

      for index in range(0,len(df)):
        abstract=None
        authors=None
        doi=None
        publisher=None
        abstract_info=None
        doi_info=None
        url_for_pdf=None
        url_for_landing_page=None
        title = str(df['title'][index])
        year = str(df['year_scopus'][index])

        # API ELSEVIER
        scopus_id = str(df['scopus_id'][index])
        result = self.api_elsevier(scopus_id) # add self
        abstract = result['abstract']
        authors = result['authors']
        doi = result['doi']
        publisher = result['publisher']
        if abstract!=None:
          abstract_info='elsevier'
        if doi!=None:
          doi_info='elsevier'

        # API UNPAYWALL
        if doi==None or authors==None:
          if doi==None:
            result = self.api_unpaywall('title',title) # add self
          else:
            result = self.api_unpaywall('doi',doi) # add self

        #   df['url_for_pdf'][index] = result['url_for_pdf']
        #   df['url_for_landing_page'][index] = result['url_for_landing_page']

          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='unpaywall'
          if authors==None:
            authors = result['authors']

        # API SPRINGER
        if abstract==None or doi==None or authors==None:
          if doi==None:
            result = self.api_springer('title',title) # add self
          else:
            result = self.api_springer('doi',doi) # add self

          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='springer'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='springer'
          if authors==None:
            authors = result['authors']

        # API CROSSREF
        if abstract==None or doi==None or authors==None:
          if doi==None:
            result = self.api_crossref('title',title) # add self
          else:
            result = self.api_crossref('doi',doi) # add self
          
          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='crossref'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='crossref'
          if authors==None:
            authors = result['authors']
        
        # API SEMANTIC
        if abstract==None or doi==None or authors==None:
          if doi==None:
            result = self.api_semantic('title',title,year) # add self
          else:
            result = self.api_semantic('doi',doi) # add self
          
          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='semantic'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='semantic'
          if authors==None:
            authors = result['authors']

        # ASSIGN TO DATAFRAME
        df['abstract'][index] = abstract
        try:
          authors = '; '.join(authors)
        except Exception:
          authors=None
        df['authors'][index] = authors
        df['doi'][index] = doi
        df['publisher'][index] = publisher
        df['abstract_info'][index] = abstract_info 
        df['doi_info'][index] = doi_info
          
      return df
    
    # COMPLETE WOS
    def complete_wos(self,df):
      df = df.loc[df['title'].notnull()].reset_index(drop=True)
      df['abstract']=None
      df['doi']=None
      df['publisher']=None
      df['type_paper']=None
      df['abstract_info']=None
      df['doi_info']=None
    #   df['url_for_pdf']=None
    #   df['url_for_landing_page']=None

      for index in range(0,len(df)):
        abstract=None
        doi=None
        publisher=None
        abstract_info=None
        doi_info=None
        type_paper=None
        url_for_pdf=None
        url_for_landing_page=None
        title = str(df['title'][index])
        year = str(df['year_wos'][index])

        # API UNPAYWALL
        result = self.api_unpaywall('title',title) # add self
        doi = result['doi']
        publisher = result['publisher']
        type_paper = result['type_paper']
        
        # df['url_for_pdf'][index] = result['url_for_pdf'] # opsional
        # df['url_for_landing_page'][index] = result['url_for_landing_page'] # opsional

        if doi!=None:
          doi_info='unpaywall'

        # API SPRINGER
        if abstract==None or doi==None or publisher==None or type_paper==None:
          if doi==None:
            result = self.api_springer('title',title) # add self
          else:
            result = self.api_springer('doi',doi) # add self

          if result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='springer'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='springer'
          if publisher==None:
            publisher = result['publisher']
          if type_paper==None:
            if type_paper=='Journal':
              type_paper = 'journal article'
            type_paper = result['type_paper']

        # API CROSSREF
        if abstract==None or doi==None or publisher==None or type_paper==None:
          if doi==None:
            result = self.api_crossref('title',title) # add self
          else:
            result = self.api_crossref('doi',doi) # add self
          
          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='crossref'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='crossref'
          if publisher==None:
            publisher = result['publisher']
          if type_paper==None:
            type_paper = result['type_paper']
        
        # API SEMANTIC
        if abstract==None or doi==None or type_paper==None:
          if doi==None:
            result = self.api_semantic('title',title,year) # add self
          else:
            result = self.api_semantic('doi',doi) # add self
          
          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='semantic'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='semantic'
          if type_paper==None:
            type_paper = result['type_paper']

        # ASSIGN TO DATAFRAME
        df['abstract'][index] = abstract
        df['doi'][index] = doi
        df['publisher'][index] = publisher
        df['abstract_info'][index] = abstract_info 
        df['doi_info'][index] = doi_info
        if type_paper:
          if type_paper=='journal-article' or type_paper=='JournalArticle':
            type_paper = 'journal article'
          elif type_paper=='proceedings-article':
            type_paper = 'proceeding article'
          elif type_paper=='peer-review':
            type_paper = 'peer review'
          elif type_paper=='Review':
            type_paper = 'review'
          elif type_paper=='CaseReport':
            type_paper = 'case report'
          type_paper = type_paper.lower()
        df['type_paper'][index] = type_paper

      for index in range(0,len(df)):
        if df['type_paper'][index]==None:
          try:
            if ('proceeding' in df['journal_wos'][index].lower()) and ('conference' in df['journal_wos'][index].lower()):
              df['type_paper'][index] = 'conference proceeding article'
            elif ('conference' in df['journal_wos'][index].lower()) and ('proceeding' not in df['journal_wos'][index].lower()):
              df['type_paper'][index] = 'conference article'
            elif ('proceeding' in df['journal_wos'][index].lower()) and ('conference' not in df['journal_wos'][index].lower()):
              df['type_paper'][index] = 'proceeding article'
            elif ('journal' in df['journal_wos'][index].lower() or 'jurnal' in df['journal_wos'][index].lower()):
              df['type_paper'][index] = 'journal article'
            elif 'seminar' in df['journal_wos'][index].lower():
              df['type_paper'][index] = 'seminar article'
            elif 'procedia' in df['journal_wos'][index].lower():
              df['type_paper'][index] = 'procedia article'
            else: 
              df['type_paper'][index] = None
          except Exception:
            df['type_paper'][index] = None

      return df
    
    # ABSTRACT WEB GARUDA
    def abstract_web_garuda(self,url_garuda):
      headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}
      try:
        response_garuda=requests.get(url_garuda,headers=headers)
        result = BeautifulSoup(response_garuda.content,'html.parser')
        try:
          abstract = result.find('div', attrs={'class':'abstract-article'}).find('xmp').text
        except Exception:
          abstract=None
      except Exception:
        abstract=None
      time.sleep(0.05)
      return abstract
    
    # COMPLETE GARUDA
    def complete_garuda(self,df):
      df = df.loc[df['title'].notnull()].reset_index(drop=True)
      df['type_paper']=None
      df['cited']=None
      df['abstract']=None
      df['abstract_info']=None
      df['doi_info']=None
    #   df['url_for_pdf']=None
    #   df['url_for_landing_page']=None

      for index in range(0,len(df)):
        try:
          if ('proceeding' in df['journal_garuda'][index].lower()) and ('conference' in df['journal_garuda'][index].lower()):
            df['type_paper'][index] = 'conference proceeding article'
          elif ('conference' in df['journal_garuda'][index].lower()) and ('proceeding' not in df['journal_garuda'][index].lower()):
            df['type_paper'][index] = 'conference article'
          elif ('proceeding' in df['journal_garuda'][index].lower()) and ('conference' not in df['journal_garuda'][index].lower()):
              df['type_paper'][index] = 'proceeding article'
          elif ('journal' in df['journal_garuda'][index].lower() or 'jurnal' in df['journal_garuda'][index].lower()):
            df['type_paper'][index] = 'journal article'
          elif 'seminar' in df['journal_garuda'][index].lower():
            df['type_paper'][index] = 'seminar article'
          elif 'procedia' in df['journal_garuda'][index].lower():
              df['type_paper'][index] = 'procedia article'
          else: 
            df['type_paper'][index] = None
        except Exception:
          df['type_paper'][index] = None
        doi = df['doi_garuda'][index]
        url_garuda = df['link_garuda'][index]
        # ABSTRACT FROM WEB GARUDA
        abstract = self.abstract_web_garuda(url_garuda)

        abstract_info=None
        if abstract!=None:
          abstract_info='garuda'
        doi_info=None
        if doi!=None:
          doi_info='garuda'
        cited=None
        type_paper=None
        url_for_pdf=None
        url_for_landing_page=None
        title = str(df['title'][index])
        year = str(df['year_garuda'])
        
        # API UNPAYWALL
        if doi==None:
          if doi==None:
            result = self.api_unpaywall('title',title) # add self
            doi = result['doi']
            if doi!=None:
              doi_info='unpaywall'
          else:
            result = self.api_unpaywall('doi',doi) # add self
          if df['type_paper'][index]==None:
            type_paper = result['type_paper']
          
        #   df['url_for_pdf'][index] = result['url_for_pdf'] # opsional
        #   df['url_for_landing_page'][index] = result['url_for_landing_page'] # opsional

        # API CROSSREF
        if abstract==None or doi==None:
          if doi==None:
            result = self.api_crossref('title',title) # add self
          else:
            result = self.api_crossref('doi',doi) # add self
          
          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='crossref'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='crossref'
          if df['type_paper'][index]==None:
            type_paper = result['type_paper']
        # API SEMANTIC
        if abstract==None or doi==None:
          if doi==None:
            result = self.api_semantic('title',title,year) # add self
          else:
            result = self.api_semantic('doi',doi) # add self
          
          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='semantic'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='semantic'
          cited = result['cited']
          if df['type_paper'][index]==None:
            type_paper = result['type_paper']

        # ASSIGN TO DATAFRAME
        df['cited'][index] = cited
        df['abstract'][index] = abstract
        df['doi_garuda'][index] = doi
        df['abstract_info'][index] = abstract_info 
        df['doi_info'][index] = doi_info

        if df['type_paper'][index]==None and type_paper!=None:
          if type_paper=='journal-article' or type_paper=='JournalArticle':
            type_paper = 'journal article'
          elif type_paper=='proceedings-article':
            type_paper = 'proceeding article'
          elif type_paper=='peer-review':
            type_paper = 'peer review'
          elif type_paper=='Review':
            type_paper = 'review'
          elif type_paper=='CaseReport':
            type_paper = 'case report'
          type_paper = type_paper.lower()
          df['type_paper'][index] = type_paper

      return df

    # COMPLETE GOOGLE
    def complete_google(self,df):
      df = df.loc[df['title'].notnull()].reset_index(drop=True)
      df['abstract']=None
      df['doi']=None
      df['publisher']=None
      df['type_paper']=None
      df['abstract_info']=None
      df['doi_info']=None
    #   df['url_for_pdf']=None
    #   df['url_for_landing_page']=None

      for index in range(0,len(df)):
        abstract=None
        authors=None
        doi=None
        publication_name=None
        publisher=None
        type_paper=None
        abstract_info=None
        doi_info=None
        url_for_pdf=None
        url_for_landing_page=None
        title = str(df['title'][index])
        year = str(df['year_google'][index])

        # API UNPAYWALL
        result = self.api_unpaywall('title',title) # add self
        doi = result['doi']
        publication_name = result['publication_name']
        publisher = result['publisher']
        type_paper = result['type_paper']

        # df['url_for_pdf'][index] = result['url_for_pdf'] # opsional
        # df['url_for_landing_page'][index] = result['url_for_landing_page'] # opsional

        if result['authors']!=None:
          authors = ', '.join(result['authors'])
          df['authors_google'][index] = authors
        if doi!=None:
          doi_info='unpaywall'

        # API CROSSREF
        if abstract==None or doi==None or type_paper==None:
          if doi==None:
            result = self.api_crossref('title',title) # add self
          else:
            result = self.api_crossref('doi',doi) # add self

          publisher = result['publisher']
          if result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='crossref'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='crossref'
          if authors==None and result['authors']!=None:
            authors = ', '.join(result['authors'])
            df['authors_google'][index] = authors
          if publication_name==None and result['publication_name']!=None:
            publication_name = result['publication_name']
          if type_paper==None:
            type_paper = result['type_paper']
        
        # API SEMANTIC
        if abstract==None or doi==None or type_paper==None:
          if doi==None:
            result = self.api_semantic('title',title,year) # add self
          else:
            result = self.api_semantic('doi',doi) # add self
          
          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='semantic'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='semantic'
          if authors==None and result['authors']!=None:
            authors = ', '.join(result['authors'])
            df['authors_google'][index] = authors
          if publication_name==None and result['publication_name']!=None:
            publication_name = result['publication_name']
          if type_paper==None:
            type_paper = result['type_paper']

        # API SPRINGER
        if abstract==None or doi==None or type_paper==None:
          if doi==None:
            result = self.api_springer('title',title) # add self
          else:
            result = self.api_springer('doi',doi) # add self

          if abstract==None and result['abstract']!=None:
            abstract = result['abstract']
            abstract_info='springer'
          if doi==None and result['doi']!=None:
            doi = result['doi']
            doi_info='springer'
          if authors==None and result['authors']!=None:
            authors = []
            for i in range(0,len(result['authors'])):
              author = result['authors'][i].split(', ')
              try:
                author = author[1]+' '+author[0]
              except Exception:
                author = author[0]
              authors.append(author)
            authors = ', '.join(authors)
            df['authors_google'][index] = authors
          if publisher==None and result['publisher']!=None:
            publisher = result['publisher']
          if publication_name==None and result['publication_name']!=None:
            publication_name = result['publication_name']
          if type_paper==None:
            if type_paper=='Journal':
              type_paper = 'journal article'
            type_paper = result['type_paper']

        # ASSIGN TO DATAFRAME
        df['abstract'][index] = abstract
        df['doi'][index] = doi
        if publication_name!=None:
          df['journal_google'][index] = publication_name
        df['publisher'][index] = publisher
        df['abstract_info'][index] = abstract_info 
        df['doi_info'][index] = doi_info
        if type_paper:
          if type_paper=='journal-article' or type_paper=='JournalArticle':
            type_paper = 'journal article'
          elif type_paper=='proceedings-article':
            type_paper = 'proceeding article'
          elif type_paper=='peer-review':
            type_paper = 'peer review'
          elif type_paper=='Review':
            type_paper = 'review'
          elif type_paper=='CaseReport':
            type_paper = 'case report'
          type_paper = type_paper.lower()
        df['type_paper'][index] = type_paper

      for index in range(0,len(df)):
        if df['type_paper'][index]==None:
          try:
            if ('proceeding' in df['journal_google'][index].lower()) and ('conference' in df['journal_google'][index].lower()):
              df['type_paper'][index] = 'conference proceeding article'
            elif ('conference' in df['journal_google'][index].lower()) and ('proceeding' not in df['journal_google'][index].lower()):
              df['type_paper'][index] = 'conference article'
            elif ('proceeding' in df['journal_google'][index].lower()) and ('conference' not in df['journal_google'][index].lower()):
              df['type_paper'][index] = 'proceeding article'
            elif ('journal' in df['journal_google'][index].lower() or 'jurnal' in df['journal_google'][index].lower()):
              df['type_paper'][index] = 'journal article'
            elif 'seminar' in df['journal_google'][index].lower():
              df['type_paper'][index] = 'seminar article'
            elif 'procedia' in df['journal_google'][index].lower():
              df['type_paper'][index] = 'procedia article'
            else: 
              df['type_paper'][index] = None
          except Exception:
            df['type_paper'][index] = None

      return df
     
class Convert(object):

    def detect_lang(self,df,col):
      lang=[]
      for sentence in df[col]:
        try:
          lang_sentence = detect(str(sentence.lower()))
        except Exception:
          lang_sentence=None
        lang.append(lang_sentence)
      return lang

    def convert_scopus(self,scopus):
    #   scopus['string_authors'] = scopus.apply(lambda row: self.list_to_string(row['authors']), axis=1)
      df_scopus = pd.DataFrame()
      df_scopus['id'] = scopus.index+1
      df_scopus['author_sinta'] = scopus['author_scopus'].values
      df_scopus['judul'] = scopus['title'].values
      df_scopus['abstrak'] = scopus['abstract'].values
      df_scopus['doi'] = scopus['doi'].values
      df_scopus['authors'] = scopus['authors'].values
      df_scopus['nama_publikasi'] = scopus['journal_scopus'].values
      df_scopus['tipe'] = scopus['type_paper'].values
      df_scopus['publisher'] = scopus['publisher'].values
      df_scopus['rank'] = scopus['rank_scopus'].values
      df_scopus['link'] = scopus['link_scopus'].values
      df_scopus['tahun'] = scopus['year_scopus'].values
      df_scopus['jumlah_sitasi'] = scopus['cited_scopus'].values
      df_scopus['abstrak_info'] = scopus['abstract_info'].values
      df_scopus['doi_info'] = scopus['doi_info'].values
      df_scopus['lang_judul'] = self.detect_lang(df_scopus,'judul')
      df_scopus['lang_abstrak'] = self.detect_lang(df_scopus,'abstrak')

      return df_scopus

    def convert_wos(self,wos):
      df_wos= pd.DataFrame()
      df_wos['id'] = wos.index+1
      df_wos['author_sinta'] = wos['author_wos'].values
      df_wos['judul'] = wos['title'].values
      df_wos['abstrak'] = wos['abstract'].values
      df_wos['doi'] = wos['doi'].values
      df_wos['authors'] = wos['authors_wos'].values
      df_wos['nama_publikasi'] = wos['journal_wos'].values
      df_wos['tipe'] = wos['type_paper'].values
      df_wos['publisher'] = wos['publisher'].values
      df_wos['rank'] = wos['rank_wos'].values
      df_wos['link'] = wos['link_wos'].values
      df_wos['tahun'] = wos['year_wos'].values
      df_wos['jumlah_sitasi'] = wos['cited_wos'].values
      df_wos['abstrak_info'] = wos['abstract_info'].values
      df_wos['doi_info'] = wos['doi_info'].values
      df_wos['lang_judul'] = self.detect_lang(df_wos,'judul')
      df_wos['lang_abstrak'] = self.detect_lang(df_wos,'abstrak')

      return df_wos

    def convert_garuda(self,garuda):
      df_garuda = pd.DataFrame()
      df_garuda['id'] = garuda.index+1
      df_garuda['author_sinta'] = garuda['author_garuda'].values
      df_garuda['judul'] = garuda['title'].values
      df_garuda['abstrak'] = garuda['abstract'].values
      df_garuda['doi'] = garuda['doi_garuda'].values
      df_garuda['authors'] = garuda['authors_garuda'].values
      df_garuda['nama_publikasi'] = garuda['journal_garuda'].values
      df_garuda['tipe'] = garuda['type_paper'].values
      df_garuda['publisher'] =  garuda['publisher'].values
      df_garuda['rank'] = garuda['rank_garuda'].values
      df_garuda['link'] = garuda['link_garuda'].values
      df_garuda['tahun'] = garuda['year_garuda'].values
      df_garuda['jumlah_sitasi'] = garuda['cited'].values
      df_garuda['abstrak_info'] = garuda['abstract_info'].values
      df_garuda['doi_info'] = garuda['doi_info'].values
      df_garuda['lang_judul'] = self.detect_lang(df_garuda,'judul')
      df_garuda['lang_abstrak'] = self.detect_lang(df_garuda,'abstrak')

      return df_garuda

    def convert_google(self,google):
      df_google = pd.DataFrame()
      df_google['id'] = google.index+1
      df_google['author_sinta'] = google['author_google'].values
      df_google['judul'] = google['title'].values
      df_google['abstrak'] = google['abstract'].values
      df_google['doi'] = google['doi'].values
      df_google['authors'] = google['authors_google'].values
      df_google['nama_publikasi'] = google['journal_google'].values
      df_google['publisher'] = google['publisher'].values
      df_google['tipe'] = google['type_paper'].values
      df_google['rank'] = None
      df_google['link'] = google['link_google'].values
      df_google['tahun'] = google['year_google'].values
      df_google['jumlah_sitasi'] = google['cited_google'].values
      df_google['abstrak_info'] = google['abstract_info'].values
      df_google['doi_info'] = google['doi_info'].values
      df_google['lang_judul'] = self.detect_lang(df_google,'judul')
      df_google['lang_abstrak'] = self.detect_lang(df_google,'abstrak')

      return df_google